{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assessing Financial Assets Behavioural Factors and Forecasting**\n"
      ],
      "metadata": {
        "id": "skqSGaqrJpID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SPRINT 1\n",
        "\n",
        "### 1- Data Collection and Research\n",
        "\n",
        "*   Find financial datasets, which can be stocks or derivatives products (such as options) with varying frequencies (minutely, hourly, daily)\n",
        "*   Do detailed research on the behavioural finance techniques (also machine learning techniques) that can be applied to stock/portfolio assessments/decisions (SentimentaL analysis, Technical analysis, Behavioural economics models, Market surveys and interviews, Investor sentiment indices)\n",
        "*   Make a list of the behavioural finance methods/models and ML/DL algorithms that can be used for the project\n",
        "\n",
        "### 2- Data Preprocessing and Exploratory Analysis\n",
        "\n",
        "*   Start cleaning the dataset\n",
        "*   Start filling in the missing data\n",
        "*   Start doing the simple statistical analysis\n",
        "*   The analysis will include various analyses such as with different normalisation methods (log scale, z-score etc), different smoothing algorithms to apply etc.\n"
      ],
      "metadata": {
        "id": "VTpaiDyAJmLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️.Importing libraries\n",
        "####\n",
        "### 2️. Data Collection – Downloading hourly S&P 500 data from yahoo finance website"
      ],
      "metadata": {
        "id": "UDNaNpY1hrIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 1: IMPORTS & DATA DOWNLOAD\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "import holidays\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy import stats\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import math"
      ],
      "metadata": {
        "id": "U_eSmoZhmsPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are doing all the processing on 1 hour interval (mid-term data), because of yahoo finance limitation we will only be able to get last 60 days of data\n",
        "\n",
        "sp500 = yf.download(\"^GSPC\", period=\"6mo\", interval=\"1h\")\n",
        "print(\"Raw shape:\", sp500.shape)\n",
        "sp500.head(300)"
      ],
      "metadata": {
        "id": "xD_qxrNJrH8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500.tail()"
      ],
      "metadata": {
        "id": "vljDj272kDeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Cleaning and Data Filling"
      ],
      "metadata": {
        "id": "UlsyQjlWhz0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️ Column cleaning / flattening\n",
        "# Handle potential MultiIndex\n",
        "if isinstance(sp500.columns, pd.MultiIndex):\n",
        "    sp500.columns = [c[0] for c in sp500.columns]\n",
        "\n",
        "# Reset index of Datetime\n",
        "sp500 = sp500.reset_index()\n",
        "sp500['Datetime'] = pd.to_datetime(sp500['Datetime'])\n",
        "sp500.rename(columns=str.strip, inplace=True)\n",
        "print(\"Columns:\", sp500.columns.tolist())"
      ],
      "metadata": {
        "id": "nfdqgI6OnTFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View data sample and structure\n",
        "print(\"Shape of Dataset\",sp500.shape)\n",
        "print(f\"Date range: {sp500['Datetime'].min()} → {sp500['Datetime'].max()}\\n\")\n",
        "print(sp500.head(10))\n",
        "print(sp500.columns)\n",
        "print(sp500.index[-1])  # last timestamp\n",
        "print(sp500.tail(10))"
      ],
      "metadata": {
        "id": "NLrkM9J1kGO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for any missing values in the dataset\n",
        "print(\"Missing Values Before Cleaning:\\n\")\n",
        "print(sp500.isna().sum().round(4))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Displaying basic statistics to spot anomalies\n",
        "print(\"Basic Statistics (Before Cleaning):\\n\")\n",
        "print(sp500.describe().round(4))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Checking for zero or negative trading volumes\n",
        "if 'Volume' in sp500.columns:\n",
        "    zero_or_negative = (sp500['Volume'] <= 0).sum()\n",
        "    print(f\"Zero or negative volumes found: {zero_or_negative:.4f}\\n\")\n",
        "\n",
        "# Checking for zero or negative trading close\n",
        "if 'Close' in sp500.columns:\n",
        "    zero_or_negative = (sp500['Close'] <= 0).sum()\n",
        "    print(f\"Zero or negative Close found: {zero_or_negative:.4f}\\n\")"
      ],
      "metadata": {
        "id": "YqmG6aB1kO72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Datetime index\n",
        "\n",
        "\n",
        "sp500['Datetime'] = pd.to_datetime(sp500['Datetime'])\n",
        "sp500 = sp500.set_index('Datetime').sort_index()\n",
        "\n",
        "# FIX VOLUME: Replace 0 with NaN → fill per trading day\n",
        "#print(\"Fixing Volume (0 → NaN → day-wise fill)...\")\n",
        "sp500['Date'] = sp500.index.date\n",
        "\n",
        "# Remove pre/post-market hours (keep only regular trading hours: 14:30–20:00 UTC)\n",
        "sp500 = sp500.between_time('14:30', '20:00')\n",
        "sp500 = sp500[sp500['Volume'] > 0]\n",
        "\n",
        "\n",
        "# Replace zero or negative volume with NaN\n",
        "zero_vol = sp500['Volume'] <= 0\n",
        "print(f\"Found {zero_vol.sum()} zero-volume rows → converting to NaN\")\n",
        "sp500.loc[zero_vol, 'Volume'] = np.nan\n",
        "\n",
        "# Fill Volume: forward + backward per trading day\n",
        "sp500['Volume'] = sp500.groupby('Date')['Volume'].transform(lambda x: x.ffill().bfill())\n",
        "\n",
        "# Final fallback: use median if any NaN remains\n",
        "if sp500['Volume'].isna().any():\n",
        "    median_vol = sp500['Volume'].median()\n",
        "    sp500['Volume'] = sp500['Volume'].fillna(median_vol)\n",
        "    print(f\"Filled remaining NaNs with median volume: {median_vol:,.0f}\")\n",
        "\n",
        "print(f\"Volume fixed! Min volume = {sp500['Volume'].min():,.0f}\\n\")"
      ],
      "metadata": {
        "id": "BcZIGXqHkRRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking for any remaining missing or invalid values\n",
        "print(\" Missing Values After Cleaning:\\n\")\n",
        "print(sp500.isna().sum())\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(\"Basic Statistics (After Cleaning):\\n\")\n",
        "print(sp500.describe().round(4))\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Confirming zero or negative volumes are gone\n",
        "if 'Volume' in sp500.columns:\n",
        "    remaining_invalid = (sp500['Volume'] <= 0).sum()\n",
        "    print(f\"Remaining zero or negative volumes: {remaining_invalid}\")"
      ],
      "metadata": {
        "id": "ylQtrCeGkTWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean Summary Output\n",
        "clean_summary = {\n",
        "    \"Initial Zero/Negative Volumes\": zero_or_negative,\n",
        "    \"Remaining Zero/Negative Volumes\": remaining_invalid,\n",
        "    \"Total Missing Values (After Cleaning)\": sp500.isna().sum().sum()\n",
        "}\n",
        "\n",
        "print(\"\\n Data Cleaning Summary:\")\n",
        "for key, value in clean_summary.items():\n",
        "    print(f\"• {key}: {value}\")"
      ],
      "metadata": {
        "id": "DyY4REdLkVSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILDING COMPLETE TRADING CALENDAR (13:30–19:30 UTC)\n",
        "print(\" Building perfect hourly trading grid...\")\n",
        "us_holidays = holidays.US()\n",
        "start = sp500.index.min().date()\n",
        "end = sp500.index.max().date()\n",
        "\n",
        "trading_days = pd.bdate_range(start, end)\n",
        "trading_days = [d for d in trading_days if d not in us_holidays]\n",
        "\n",
        "full_idx = []\n",
        "for day in trading_days:\n",
        "    ts = pd.Timestamp(day).replace(hour=13, minute=30, tzinfo=sp500.index.tz)\n",
        "    hours = pd.date_range(ts, periods=7, freq='H')\n",
        "    full_idx.extend(hours)\n",
        "\n",
        "expected_index = pd.DatetimeIndex(full_idx)\n",
        "print(f\"   Expected bars: {len(expected_index)}\\n\")"
      ],
      "metadata": {
        "id": "G0uzStVfkXVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REINDEX & FILL GAPS\n",
        "print(\"Reindexing to full grid & filling gaps...\")\n",
        "sp500 = sp500.reindex(expected_index)\n",
        "\n",
        "# Forward-fill prices\n",
        "price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "sp500[price_cols] = sp500[price_cols].ffill()\n",
        "\n",
        "# Forward-fill volume\n",
        "sp500['Volume'] = sp500['Volume'].ffill()\n",
        "\n",
        "print(f\"   Final shape: {sp500.shape}\")\n",
        "print(f\"   Missing values: {sp500.isna().sum().sum()}\\n\")"
      ],
      "metadata": {
        "id": "3HOE5JI2kZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINAL GAP CHECK\n",
        "gaps = sp500.index.to_series().diff().dt.total_seconds() / 3600\n",
        "gaps.iloc[0] = 1\n",
        "\n",
        "trading = gaps[gaps <= 24]\n",
        "weekend = gaps[gaps > 24]\n",
        "\n",
        "print(\"\\nFINAL RESULT:\")\n",
        "print(f\"   Trading gaps: {len(trading)}\")\n",
        "print(f\"   Weekend gaps: {len(weekend)}\")\n",
        "print(f\"   MAX TRADING GAP: {trading.max():.2f}h\")\n",
        "print(f\"   Weekend gap: {weekend.head(1).iloc[0]:.1f}h\")"
      ],
      "metadata": {
        "id": "F0Q0jlyYkbNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify every day has 7 bars\n",
        "daily = sp500.groupby(sp500.index.date).size()\n",
        "print(\"Bars per day:\", daily.value_counts().to_dict())\n",
        "print(\"Total days:\", len(daily))\n",
        "print(\"Total bars:\", len(sp500))"
      ],
      "metadata": {
        "id": "Fk9UdQLrkdOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure index is datetime\n",
        "sp500.index = pd.to_datetime(sp500.index)\n",
        "\n",
        "# 1) Infer expected frequency\n",
        "expected_freq = pd.infer_freq(sp500.index)\n",
        "if expected_freq is None:\n",
        "    # fallback: use mode of deltas (most common difference)\n",
        "    td = sp500.index.to_series().diff().dropna()\n",
        "    expected_freq = td.mode().iloc[0]  # a Timedelta\n",
        "    # convert Timedelta to pandas offset string, e.g. 'H', '30T', etc.\n",
        "    # Try convenient conversions:\n",
        "    if expected_freq % pd.Timedelta(minutes=60) == pd.Timedelta(0):\n",
        "        hours = int(expected_freq / pd.Timedelta(hours=1))\n",
        "        expected_freq = f'{hours}h'\n",
        "    elif expected_freq % pd.Timedelta(minutes=30) == pd.Timedelta(0):\n",
        "        mins = int(expected_freq / pd.Timedelta(minutes=1))\n",
        "        expected_freq = f'{mins}T'\n",
        "    else:\n",
        "        # generic fallback to timedeltas string\n",
        "        expected_freq = str(td.mode().iloc[0])\n",
        "\n",
        "print(\"Inferred expected frequency:\", expected_freq)\n",
        "\n",
        "# 2) Build full expected index (between min/max)\n",
        "us_holidays = holidays.US()\n",
        "\n",
        "full_range = pd.date_range(start=sp500.index.min(),\n",
        "                           end=sp500.index.max(),\n",
        "                           freq=expected_freq)\n",
        "\n",
        "# Filter out weekends and US holidays (keeps only Mon-Fri and non-holiday dates)\n",
        "full_range = [ts for ts in full_range if ts.weekday() < 5 and ts.date() not in us_holidays]\n",
        "full_range = pd.DatetimeIndex(full_range)\n",
        "\n",
        "print(\"Length of original index:\", len(sp500.index))\n",
        "print(\"Length of expected full_range:\", len(full_range))\n",
        "\n",
        "# 3) Detect gaps (timestamps in expected range missing from sp500)\n",
        "missing = full_range.difference(sp500.index)\n",
        "print(\"\\nNumber of missing timestamps (expected but not present):\", len(missing))\n",
        "\n",
        "# Show the first few missing timestamps (if any)\n",
        "if len(missing) > 0:\n",
        "    display(pd.Series(missing).head(20))  # shows up to first 20 missing\n",
        "else:\n",
        "    print(\"No missing timestamps found in expected trading timeline.\")\n",
        "\n",
        "# Optional: show 'gaps' by looking at actual time diffs > expected\n",
        "# (this is useful if expected_freq is not exact string)\n",
        "time_diff = sp500.index.to_series().diff().dropna()\n",
        "# convert expected_freq into a Timedelta for comparison\n",
        "try:\n",
        "    expected_td = pd.to_timedelta(pd.tseries.frequencies.to_offset(expected_freq))\n",
        "except Exception:\n",
        "    # fallback: use mode diff\n",
        "    expected_td = sp500.index.to_series().diff().mode().iloc[0]\n",
        "\n",
        "large_gaps = time_diff[time_diff > expected_td]\n",
        "print(\"\\nDetected large gaps in raw data (diff > expected):\", large_gaps.shape[0])\n",
        "if not large_gaps.empty:\n",
        "    display(large_gaps.head(20))\n",
        "\n",
        "# 4) Reindex to full_range and fill missing values\n",
        "sp500_reindexed = sp500.reindex(full_range)\n",
        "\n",
        "# Use forward-fill\n",
        "sp500_filled = sp500_reindexed.ffill()  # or .bfill() or .interpolate(method='time')\n",
        "\n",
        "# 5) Re-check — there should be no missing expected timestamps\n",
        "missing_after = full_range.difference(sp500_filled.index)\n",
        "print(\"\\nMissing after reindex & ffill:\", len(missing_after))\n",
        "\n",
        "nan_counts_after = sp500_filled.isna().sum()\n",
        "print(\"\\nNaN counts after filling (per column):\\n\", nan_counts_after[nan_counts_after > 0])\n",
        "\n",
        "print(\"\\nSample rows around first previously missing timestamp:\")\n",
        "if len(missing) > 0:\n",
        "    ts = missing[0]\n",
        "    display(sp500_filled.loc[ts - pd.Timedelta(expected_freq) : ts + pd.Timedelta(expected_freq)].head(10))\n",
        "else:\n",
        "    print(\"Nothing to show — no missing timestamps originally.\")\n"
      ],
      "metadata": {
        "id": "XqP7lBlvkfA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"5. FINAL GAP CHECK – Should show ZERO gaps\")\n",
        "time_diff = sp500.index.to_series().diff().fillna(pd.Timedelta(hours=1))\n",
        "gaps = time_diff[time_diff > pd.Timedelta(hours=1)]\n",
        "\n",
        "if len(gaps) == 0:\n",
        "    print(\"   PERFECT! No gaps. Data is continuous every hour.\")\n",
        "else:\n",
        "    print(f\"   Found {len(gaps)} gaps (should not happen):\")\n",
        "    print(gaps.head())\n",
        "\n",
        "# Plot continuity\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(sp500.index, time_diff.dt.total_seconds()/3600,\n",
        "         color='red', linewidth=2, label='Gap (hours)')\n",
        "plt.axhline(1, color='gray', linestyle='--', alpha=0.7)\n",
        "plt.title('Hourly Continuity Graph')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Hours Between Bars')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zTAYUiTmkhF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7, 4))\n",
        "plt.plot(sp500.index, sp500['Close'], color='navy', linewidth=1.5)\n",
        "plt.title('S&P 500 Time Series Trend')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2j_69g7GkkPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Normalization Methods"
      ],
      "metadata": {
        "id": "WNE8df_OiP3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# B. Z-score normalization (StandardScaler)\n",
        "scaler_z = StandardScaler()\n",
        "sp500['Close_zscore'] = scaler_z.fit_transform(sp500[['Close']])\n",
        "\n",
        "# C. Min-Max normalization (0–1 scaling)\n",
        "scaler_mm = MinMaxScaler(feature_range=(0,1))\n",
        "sp500['Close_minmax'] = scaler_mm.fit_transform(sp500[['Close']])\n",
        "\n",
        "# Compare visually\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(sp500.index, sp500['Close_zscore'], label='Z-score Normalized')\n",
        "plt.plot(sp500.index, sp500['Close_minmax'], label='MinMax Scaled')\n",
        "# plt.plot(sp500.index, sp500['Close_log'], label='Log Scaled')\n",
        "plt.title('Normalized Close Prices (Z-score vs MinMax)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EV7ds3B8kmgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Smoothing Techniques"
      ],
      "metadata": {
        "id": "W7MP_Kh9iWMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A. Simple Moving Average (SMA)\n",
        "sp500['SMA_5'] = sp500['Close'].rolling(window=5).mean()\n",
        "sp500['SMA_20'] = sp500['Close'].rolling(window=20).mean()\n",
        "\n",
        "# B. Exponential Moving Average (EMA)\n",
        "sp500['EMA_5'] = sp500['Close'].ewm(span=5, adjust=False).mean()\n",
        "sp500['EMA_20'] = sp500['Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "# C. Savitzky–Golay Filter (window=11, polyorder=2)\n",
        "sp500['Close_savgol'] = savgol_filter(sp500['Close'], window_length=11, polyorder=2)\n",
        "\n",
        "# Plot comparison of smoothing methods\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.plot(sp500.index, sp500['Close'], label='Original', alpha=0.6)\n",
        "plt.plot(sp500.index, sp500['SMA_5'], label='SMA 5')\n",
        "plt.plot(sp500.index, sp500['EMA_5'], label='EMA 5')\n",
        "plt.plot(sp500.index, sp500['Close_savgol'], label='Savitzky–Golay')\n",
        "plt.title('Smoothing Methods Comparison')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BocrgP1xkzHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Descriptive Statistics Method\n"
      ],
      "metadata": {
        "id": "zOS_nfRelMXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desc = sp500['Close'].describe()\n",
        "skewness, kurt = sp500['Close'].skew(), sp500['Close'].kurt()\n",
        "print(\"Descriptive statistics:\\n\", desc)\n",
        "print(f\"Skewness: {skewness:.4f}, Kurtosis: {kurt:.4f}\")"
      ],
      "metadata": {
        "id": "XBM2FJmzlEao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Return and Volatility"
      ],
      "metadata": {
        "id": "Hdk87ajpjMGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp500['Return'] = sp500['Close'].pct_change()\n",
        "print(f\"\\nSTATISTICS\")\n",
        "print(f\"Mean hourly return : {sp500['Return'].mean():.6%}\")\n",
        "print(f\"Volatility (std)   : {sp500['Return'].std():.6%}\")\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.hist(sp500['Return'].dropna(), bins=80, color='skyblue', edgecolor='black', alpha=0.8)\n",
        "plt.title('Hourly Returns Distribution')\n",
        "plt.xlabel('Return')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L4-ML6ISlG0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Outlier Detection (z-score and IQR)\n"
      ],
      "metadata": {
        "id": "Vo-xG422lh7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Z-score approach on log returns\n",
        "sp500['Return'] = sp500['Close'].pct_change()\n",
        "sp500['Log_Return'] = np.log(sp500['Close'] / sp500['Close'].shift(1))\n",
        "z_scores = np.abs(stats.zscore(sp500['Log_Return'].dropna()))\n",
        "z_outliers_idx = sp500['Log_Return'].dropna().index[z_scores > 3]\n",
        "\n",
        "# IQR approach on Close\n",
        "Q1 = sp500['Close'].quantile(0.25)\n",
        "Q3 = sp500['Close'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower = Q1 - 1.5*IQR\n",
        "upper = Q3 + 1.5*IQR\n",
        "iqr_outliers_idx = sp500[(sp500['Close'] < (Q1 - 1.5*IQR)) | (sp500['Close'] > (Q3 + 1.5*IQR))].index\n",
        "\n",
        "# Plotting close with outliers highlighted\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(sp500['Close'], label='Close')\n",
        "plt.scatter(z_outliers_idx, sp500.loc[z_outliers_idx, 'Close'], color='red', s=20, label='Return Z-outliers')\n",
        "# plt.scatter(iqr_outliers_idx, sp500.loc[iqr_outliers_idx, 'Close'], color='green', s=20, label='Price IQR-outliers')\n",
        "plt.title('Close with detected outliers')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "e7ExNAT0lXmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Final Dataset Review"
      ],
      "metadata": {
        "id": "3PpG_avpnscc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFINAL DATASET READY!\")\n",
        "print(f\"Shape : {sp500.shape}\")\n",
        "print(f\"Columns: {list(sp500.columns)}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(sp500.head())\n",
        "\n",
        "print(\"Last 5 rows:\")\n",
        "display(sp500.tail())"
      ],
      "metadata": {
        "id": "Dr8a5ysqlj6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPRINT 2\n",
        "\n",
        "\n",
        "\n",
        "## 1.   Initial Model Building\n",
        "\n",
        "*   Get the first results of the collected data in the form of summary  statistics, the accuracy of the model etc.\n",
        "*   Tweak the current model’s parameters to see different outcomes\n",
        "*   Start using another algorithm to compare the results with the first model\n",
        "outcome.\n",
        "\n",
        "## 2.  First Comparison of Different Models\n",
        "\n",
        "\n",
        "*  Improve the current models’ outcome (pay attention to the normalisation part)\n",
        "*   Re-organise the code where the results can be seen at one glance with comparisons\n",
        "(Complete results of each model with only the essential charts)\n",
        "*  Revise the platforms can be used for the overall illustration of the different models"
      ],
      "metadata": {
        "id": "Jc1osjrsv7uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# MODEL 1: ARIMA\n",
        "# =============================================================================\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "df_ARIMA = sp500.copy()\n",
        "print(\"\\nTraining ARIMA(5,0,5)\")\n",
        "# Use the same training log-returns\n",
        "# Create returns\n",
        "df_ARIMA['Return'] = df_ARIMA['Close'].pct_change()\n",
        "df_ARIMA['Log_Return'] = np.log(df_ARIMA['Close']).diff()\n",
        "\n",
        "# === TARGET = NEXT HOUR LOG RETURN\n",
        "df_ARIMA['target'] = df_ARIMA['Log_Return'].shift(-1)\n",
        "\n",
        "# Train/test split (80/20)\n",
        "split_ARIMA = int(0.8 * len(df_ARIMA))\n",
        "train_ARIMA= df_ARIMA.iloc[:split_ARIMA]\n",
        "test_ARIMA = df_ARIMA.iloc[split_ARIMA:]\n",
        "\n",
        "X_train_ARIMA = train_ARIMA.drop(['Close', 'Return', 'Log_Return', 'target'], axis=1)\n",
        "y_train_ARIMA = train_ARIMA['target']\n",
        "X_test_ARIMA = test_ARIMA.drop(['Close', 'Return', 'Log_Return', 'target'], axis=1)\n",
        "y_test_ARIMA = test_ARIMA['target']\n",
        "\n",
        "train_returns_ARIMA = train_ARIMA['Log_Return'].values\n",
        "arima_model = ARIMA(train_returns_ARIMA, order=(5,0,5))\n",
        "arima_fitted = arima_model.fit()\n",
        "\n",
        "# Forecast next hour returns for the test period\n",
        "arima_pred_returns = arima_fitted.forecast(steps=len(test_ARIMA))\n",
        "\n",
        "# Convert back to price\n",
        "# Use the previous close prices, but drop the first NaN from shift\n",
        "prev_close = test_ARIMA['Close'].shift(1).dropna()  # length is len(test_ARIMA)-1\n",
        "arima_price_pred = prev_close * np.exp(arima_pred_returns[:-1])  # align lengths\n",
        "\n",
        "# Align test prices (drop the first row)\n",
        "y_test_price = test_ARIMA['Close'].iloc[1:]\n",
        "\n",
        "mae_arima = mean_absolute_error(y_test_price, arima_price_pred)\n",
        "rmse_arima = np.sqrt(mean_squared_error(y_test_price, arima_price_pred))\n",
        "r2_arima = r2_score(y_test_price, arima_price_pred)\n",
        "mape_arima = np.mean(np.abs((y_test_price - arima_price_pred) / y_test_price)) * 100\n",
        "\n",
        "print(\"ARIMA(5,0,5) RESULT\")\n",
        "print(f\"{'MAE':>10}: {mae_arima:.3f}\")\n",
        "print(f\"{'RMSE':>10}: {rmse_arima:.3f}\")\n",
        "print(f\"{'R²':>10}: {r2_arima:.6f}\")\n",
        "print(f\"{'MAPE':>10}: {mape_arima:.3f}%\")"
      ],
      "metadata": {
        "id": "xnHP6tdvlmVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "actual_prices_arima = test_ARIMA['Close'].iloc[1:]\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(actual_prices_arima.index, actual_prices_arima, label='Actual', linewidth=2)\n",
        "plt.plot(actual_prices_arima.index, arima_price_pred, label='ARIMA Prediction', linestyle='--')\n",
        "plt.title(\"ARIMA Predicted Prices vs Actual Prices\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mqxj_Fr3lReS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# XGBoost\n",
        "# =============================================================================\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "print(\"XGBoost v2: Predicting Log Returns\")\n",
        "\n",
        "# Start from your cleaned data\n",
        "df_xgb = sp500.copy()\n",
        "\n",
        "# ==================== TARGET: LOG RETURNS ====================\n",
        "df_xgb['Log_Return'] = np.log(df_xgb['Close'] / df_xgb['Close'].shift(1))\n",
        "df_xgb = df_xgb.dropna()  # First row lost\n",
        "\n",
        "# ==================== FEATURE ENGINEERING ====================\n",
        "for lag in range(1, 16):\n",
        "    df_xgb[f'Close_lag_{lag}'] = df_xgb['Close'].shift(lag)\n",
        "    df_xgb[f'Return_lag_{lag}'] = df_xgb['Log_Return'].shift(lag)\n",
        "\n",
        "# Momentum & volatility\n",
        "df_xgb['MA_5'] = df_xgb['Close'].rolling(5).mean()\n",
        "df_xgb['MA_20'] = df_xgb['Close'].rolling(20).mean()\n",
        "df_xgb['Vol_10'] = df_xgb['Log_Return'].rolling(10).std()\n",
        "df_xgb['Vol_20'] = df_xgb['Log_Return'].rolling(20).std()\n",
        "\n",
        "# RSI (proper)\n",
        "delta = df_xgb['Close'].diff()\n",
        "gain = delta.clip(lower=0).rolling(14).mean()\n",
        "loss = -delta.clip(upper=0).rolling(14).mean()\n",
        "rs = gain / loss\n",
        "df_xgb['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# Volume + time features\n",
        "df_xgb['Volume_lag_1'] = df_xgb['Volume'].shift(1)\n",
        "df_xgb['Volume_MA_5'] = df_xgb['Volume'].rolling(5).mean()\n",
        "df_xgb['Hour'] = df_xgb.index.hour\n",
        "df_xgb['DayOfWeek'] = df_xgb.index.dayofweek\n",
        "\n",
        "# Drop NaN\n",
        "df_xgb = df_xgb.dropna()\n",
        "\n",
        "print(f\"Final dataset: {df_xgb.shape[0]} rows, {df_xgb.shape[1]} features\")\n",
        "\n",
        "# ==================== TRAIN/TEST SPLIT ====================\n",
        "train_ratio = 0.80\n",
        "split_idx = int(len(df_xgb) * train_ratio)\n",
        "\n",
        "train = df_xgb.iloc[:split_idx].copy()\n",
        "test  = df_xgb.iloc[split_idx:].copy()\n",
        "\n",
        "# Features (exclude price, returns, and future-looking columns)\n",
        "exclude = ['Close', 'Open', 'High', 'Low', 'Volume', 'Log_Return']\n",
        "feature_cols = [c for c in df_xgb.columns if c not in exclude]\n",
        "\n",
        "X_train = train[feature_cols].select_dtypes(include=[np.number])\n",
        "X_test  = test[feature_cols].select_dtypes(include=[np.number])\n",
        "y_train = train['Log_Return']\n",
        "y_test  = test['Log_Return']\n",
        "\n",
        "# ==================== TRAIN XGBOOST ON LOG RETURNS ====================\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
        "\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'eval_metric': 'mae',\n",
        "    'max_depth': 8,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.85,\n",
        "    'colsample_bytree': 0.85,\n",
        "    'min_child_weight': 5,\n",
        "    'gamma': 0.1,\n",
        "    'seed': 42,\n",
        "    'tree_method': 'hist'\n",
        "}\n",
        "\n",
        "print(\"\\nTraining on LOG RETURNS\")\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=3000,\n",
        "    evals=[(dtest, 'test')],\n",
        "    early_stopping_rounds=150,\n",
        "    verbose_eval=100\n",
        ")\n",
        "\n",
        "# ==================== PREDICT → CONVERT TO PRICE ====================\n",
        "pred_log_return = bst.predict(dtest)\n",
        "\n",
        "# Convert log return → price\n",
        "last_known_price = test['Close'].shift(1).fillna(method='bfill')\n",
        "pred_price_xgb = last_known_price * np.exp(pred_log_return)\n",
        "\n",
        "actual_price_xgb = test['Close']\n",
        "\n",
        "# ==================== FINAL METRICS (ON PRICE) ====================\n",
        "mae_xgb = mean_absolute_error(actual_price_xgb, pred_price_xgb)\n",
        "rmse_xgb = np.sqrt(mean_squared_error(actual_price_xgb, pred_price_xgb))\n",
        "r2_xgb = r2_score(actual_price_xgb, pred_price_xgb)\n",
        "mape_xgb = np.mean(np.abs((actual_price_xgb - pred_price_xgb) / actual_price_xgb)) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"XGBOOST v2 – LOG RETURN MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'MAE':<8}: {mae_xgb:8.3f}\")\n",
        "print(f\"{'RMSE':<8}: {rmse_xgb:8.3f}\")\n",
        "print(f\"{'R²':<8}: {r2_xgb:8.6f}\")\n",
        "print(f\"{'MAPE':<8}: {mape_xgb:8.3f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Save for plotting\n",
        "test = test.copy()\n",
        "test['Pred_Price_XGB'] = pred_price_xgb\n",
        "test['Actual_Price'] = actual_price_xgb"
      ],
      "metadata": {
        "id": "PE70li8RmGaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.plot(actual_price_xgb.index, actual_price_xgb, label='Actual', linewidth=2)\n",
        "plt.plot(actual_price_xgb.index, pred_price_xgb, label='XGBoost Prediction', linestyle='--')\n",
        "plt.title(\"XGBoost Predicted Prices vs Actual Prices\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qGFP6fBNl33B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LINEAR REGRESSION\n",
        "# =============================================================================\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "print(\"Linear Regression\")\n",
        "\n",
        "# Start fresh\n",
        "df_LR = sp500.copy()\n",
        "\n",
        "# Target: NEXT HOUR return\n",
        "df_LR['Future_Return'] = np.log(df_LR['Close'].shift(-1) / df_LR['Close'])\n",
        "df_LR = df_LR.dropna()  # Remove last row\n",
        "\n",
        "# Features from PAST only (lags 2 to 15 → no lag_1!)\n",
        "for lag in range(2, 16):\n",
        "    df_LR[f'Close_lag_{lag}'] = df_LR['Close'].shift(lag)\n",
        "    df_LR[f'Volume_lag_{lag}'] = df_LR['Volume'].shift(lag)\n",
        "\n",
        "df_LR['Return_lag_2'] = df_LR['Future_Return'].shift(2)\n",
        "df_LR['Return_lag_3'] = df_LR['Future_Return'].shift(3)\n",
        "df_LR['Return_lag_5'] = df_LR['Future_Return'].shift(5)\n",
        "\n",
        "df_LR['MA_5']  = df_LR['Close'].rolling(5).mean()\n",
        "df_LR['MA_20'] = df_LR['Close'].rolling(20).mean()\n",
        "df_LR['Vol_10'] = df_LR['Future_Return'].rolling(10).std()\n",
        "df_LR['RSI'] = 100 - 100/(1 + (df_LR['Close'].pct_change().rolling(14).apply(lambda x: x[x>0].mean()/-x[x<0].mean() if x[x<0].mean() != 0 else 999)))\n",
        "\n",
        "df_LR['Hour'] = df_LR.index.hour\n",
        "\n",
        "# DROP ALL ROWS WITH ANY NaN\n",
        "df_LR = df_LR.dropna()\n",
        "\n",
        "# FINAL FEATURE SELECTION — NO CURRENT OR FUTURE DATA\n",
        "feature_cols = [c for c in df_LR.columns if any(x in c for x in ['lag_', 'MA_', 'Vol_', 'RSI', 'Hour'])]\n",
        "\n",
        "X = df_LR[feature_cols].select_dtypes('number')\n",
        "y = df_LR['Future_Return']\n",
        "\n",
        "# Train/test split\n",
        "split = int(len(df_LR) * 0.8)\n",
        "X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
        "y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s = scaler.transform(X_test)\n",
        "\n",
        "# Train\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_s, y_train)\n",
        "pred_return = model.predict(X_test_s)\n",
        "\n",
        "# Convert to price — USING ONLY PAST PRICE\n",
        "actual_price = df_LR['Close'].iloc[split:split + len(pred_return)]\n",
        "prev_price = df_LR['Close'].iloc[split-1:split + len(pred_return)-1].values  # ← yesterday's close\n",
        "pred_price = prev_price * np.exp(pred_return)\n",
        "\n",
        "# Metrics\n",
        "mae_LR = mean_absolute_error(actual_price, pred_price)\n",
        "rmse_LR = np.sqrt(mean_squared_error(actual_price, pred_price))\n",
        "r2_LR = r2_score(actual_price, pred_price)\n",
        "mape_LR = np.mean(np.abs((actual_price - pred_price)/actual_price)) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LINEAR REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"MAE     : {mae_LR:.3f}\")\n",
        "print(f\"RMSE    : {rmse_LR:.3f}\")\n",
        "print(f\"R²      : {r2_LR:.6f}\")\n",
        "print(f\"MAPE    : {mape_LR:.3f}%\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "Fna0Q6K8n5_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot ACTUAL prices\n",
        "plt.plot(\n",
        "    actual_price.index,\n",
        "    actual_price.values,\n",
        "    label='Actual Price',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "# Plot PREDICTED prices (use same index!)\n",
        "plt.plot(\n",
        "    actual_price.index,\n",
        "    pred_price,\n",
        "    label='Linear Regression Forecast',\n",
        "    linestyle='--',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "plt.title(\"Linear Regression Forecast vs Actual S&P 500 Prices\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gM2VhRni5ymo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# LSTM — FUTURE FORECASTING\n",
        "# ===============================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"LSTM Forecasting Model\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 1. PREPARE DATA (ASSUME sp500 IS ALREADY CLEANED)\n",
        "# ---------------------------------------------------------------\n",
        "df = sp500.copy()\n",
        "df['Date'] = pd.to_datetime(df.index)\n",
        "df = df.set_index('Date')\n",
        "\n",
        "# Compute log returns (target for LSTM)\n",
        "df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. CREATE FEATURES\n",
        "# ---------------------------------------------------------------\n",
        "for lag in range(1, 16):\n",
        "    df[f'Close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df[f'Return_lag_{lag}'] = df['Log_Return'].shift(lag)\n",
        "\n",
        "df['MA_5'] = df['Close'].rolling(5).mean()\n",
        "df['MA_20'] = df['Close'].rolling(20).mean()\n",
        "df['Vol_10'] = df['Log_Return'].rolling(10).std()\n",
        "\n",
        "delta = df['Close'].diff()\n",
        "gain = delta.clip(lower=0).rolling(14).mean()\n",
        "loss = -delta.clip(upper=0).rolling(14).mean()\n",
        "df['RSI'] = 100 - (100 / (1 + gain / loss))\n",
        "\n",
        "df['Volume_lag_1'] = df['Volume'].shift(1)\n",
        "df['Volume_MA_5'] = df['Volume'].rolling(5).mean()\n",
        "df['Hour'] = df.index.hour\n",
        "df['DayOfWeek'] = df.index.dayofweek\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3. SELECT FEATURES AND SCALE\n",
        "# ---------------------------------------------------------------\n",
        "feature_cols = [c for c in df.columns if c not in ['Close', 'Open', 'High', 'Low', 'Volume', 'Log_Return']]\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "y = df['Log_Return'].values\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 4. CREATE SEQUENCES\n",
        "# ---------------------------------------------------------------\n",
        "TIME_STEPS = 60\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for i in range(TIME_STEPS, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i-TIME_STEPS:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 5. TRAIN/TEST SPLIT\n",
        "# ---------------------------------------------------------------\n",
        "split = int(len(X_seq) * 0.80)\n",
        "X_train, X_test = X_seq[:split], X_seq[split:]\n",
        "y_train, y_test = y_seq[:split], y_seq[split:]\n",
        "\n",
        "test_prices = df['Close'].iloc[split+TIME_STEPS:].values\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 6. BUILD LSTM MODEL\n",
        "# ---------------------------------------------------------------\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5)\n",
        "]\n",
        "\n",
        "print(\"Training LSTM...\")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 7. EVALUATE MODEL (Convert log returns → prices)\n",
        "# ---------------------------------------------------------------\n",
        "pred_log_ret_LSTM = model.predict(X_test).flatten()\n",
        "last_prices_LSTM = df['Close'].iloc[split+TIME_STEPS-1:-1].values\n",
        "pred_prices_LSTM = last_prices_LSTM * np.exp(pred_log_ret_LSTM)\n",
        "\n",
        "mae_LSTM = mean_absolute_error(test_prices, pred_prices_LSTM)\n",
        "rmse_LSTM = np.sqrt(mean_squared_error(test_prices, pred_prices_LSTM))\n",
        "r2_LSTM = r2_score(test_prices, pred_prices_LSTM)\n",
        "mape_LSTM = np.mean(np.abs((test_prices - pred_prices_LSTM) / test_prices)) * 100\n",
        "test_index_LSTM = df.index[split + TIME_STEPS:]\n",
        "\n",
        "\n",
        "print(\"\\nMODEL PERFORMANCE\")\n",
        "print(f\"MAE  = {mae_LSTM:.3f}\")\n",
        "print(f\"RMSE = {rmse_LSTM:.3f}\")\n",
        "print(f\"R²   = {r2_LSTM:.4f}\")\n",
        "print(f\"MAPE   = {mape_LSTM:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "K6hWqGovlMDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "plt.plot(\n",
        "    test_index_LSTM,\n",
        "    test_prices,\n",
        "    label='Actual Price',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "plt.plot(\n",
        "    test_index_LSTM,\n",
        "    pred_prices_LSTM,\n",
        "    label='LSTM Prediction',\n",
        "    linestyle='--',\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "plt.title(\"LSTM Predicted vs Actual S&P 500 Prices\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vbfb_I9k9Ktk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# ARIMA\n",
        "plt.plot(y_test_price.index, y_test_price, label='Actual', color='black', linewidth=2)\n",
        "plt.plot(y_test_price.index, arima_price_pred, label='ARIMA', linestyle='--')\n",
        "\n",
        "# XGBoost\n",
        "plt.plot(actual_price_xgb.index, pred_price_xgb, label='XGBoost', linestyle='--')\n",
        "\n",
        "# LSTM\n",
        "plt.plot(test_index_LSTM, pred_prices_LSTM, label='LSTM', linestyle='--')\n",
        "\n",
        "# Linear Regression\n",
        "plt.plot(actual_price.index, pred_price, label='Linear Regression', linestyle='--')\n",
        "\n",
        "\n",
        "plt.title(\"Actual vs Predicted Prices (All Models)\", fontsize=16)\n",
        "plt.xlabel(\"Time\", fontsize=12)\n",
        "plt.ylabel(\"Close Price\", fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3gBJSjtQosTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# FINAL COMPARISON TABLE\n",
        "# ============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# --- COLLECT RESULTS FROM ALL MODELS (TEST SET ONLY) ---\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['LSTM (Hourly)', 'XGBoost', 'ARIMA', 'Linear Regression'],\n",
        "    'MAE':  [mae_LSTM, mae_xgb, mae_arima, mae_LR],\n",
        "    'RMSE': [rmse_LSTM, rmse_xgb, rmse_arima, rmse_LR],\n",
        "    'R²':   [r2_LSTM, r2_xgb, r2_arima, r2_LR],\n",
        "    'MAPE (%)': [mape_LSTM, mape_xgb, mape_arima, mape_LR]\n",
        "})\n",
        "\n",
        "# --- FORMAT TABLE ---\n",
        "results = results.round(5)\n",
        "results = results.sort_values('R²', ascending=False).reset_index(drop=True)\n",
        "results.insert(0, 'Rank', range(1, len(results) + 1))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 90)\n",
        "print(\"          FINAL MODEL COMPARISON – S&P 500 FORECASTING\")\n",
        "print(\"=\" * 90)\n",
        "\n",
        "display(\n",
        "    results.style\n",
        "    .background_gradient(cmap='RdYlGn', subset=['R²'])\n",
        "    .format({\n",
        "        'MAE': '{:.3f}',\n",
        "        'RMSE': '{:.3f}',\n",
        "        'R²': '{:.6f}',\n",
        "        'MAPE (%)': '{:.3f}'\n",
        "    })\n",
        "    .set_properties(**{'font-size': '14pt', 'text-align': 'center'})\n",
        "    .set_caption(\"\")\n",
        ")\n",
        "\n",
        "print(\"=\" * 90)\n"
      ],
      "metadata": {
        "id": "stXWf_BIlGxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "     'Model': ['ARIMA(5,0,5)', 'XGBoost', 'LSTM','Linear Regression'],\n",
        "    'MAPE (%)': [mape_arima, mape_xgb, mape_LSTM, mape_LR ]\n",
        "})\n",
        "plt.figure(figsize=(14,8))\n",
        "ax = results.set_index('Model')[['MAPE (%)']].plot(kind='bar', color='orange')\n",
        "plt.title(\"Model Comparison based on MAPE \")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"MAPE (%)\")\n",
        "plt.grid(True, axis='y')\n",
        "# --- Add values on top of bars ---\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.4f')   # format to 3 decimal places\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sP3rmMmV2-a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "     'Model': ['ARIMA(5,0,5)', 'XGBoost', 'LSTM','Linear Regression'],\n",
        "    'R2 Score': [r2_arima, r2_xgb, r2_LSTM, r2_LR]\n",
        "})\n",
        "plt.figure(figsize=(14,8))\n",
        "ax = results.set_index('Model')[['R2 Score']].plot(kind='bar', figsize=(10,8), color='Green')\n",
        "plt.title(\"Model Comparison based on R2 Score\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"R² Score\")\n",
        "plt.grid(True, axis='y')\n",
        "# --- Add values on top of bars ---\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fmt='%.4f')   # format to 3 decimal places\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A6pFovgOtZbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Comparison table\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['ARIMA(5,0,5)', 'XGBoost', 'LSTM','Linear Regression'],\n",
        "    'MAE': [mae_arima, mae_xgb, mae_LSTM, mae_LR],\n",
        "    'RMSE': [rmse_arima, rmse_xgb, rmse_LSTM, rmse_LR],\n",
        "    'R2 Score': [r2_arima, r2_xgb, r2_LSTM, r2_LR]\n",
        "})\n",
        "\n",
        "print(\"\\n=== MODEL PERFORMANCE COMPARISON ===\\n\")\n",
        "print(results)\n",
        "\n",
        "# --- Bar Chart for MAE / RMSE Comparison ---\n",
        "plt.figure(figsize=(10,8))\n",
        "results.set_index('Model')[['MAE','RMSE']].plot(kind='bar', figsize=(8,6))\n",
        "plt.title(\"Model Comparison: MAE & RMSE\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fgPJ6aCrtn7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = sp500.copy()\n",
        "\n",
        "# Make sure index is datetime\n",
        "if not isinstance(df.index, pd.DatetimeIndex):\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "tzinfo = df.index.tz  # remember original timezone if any\n",
        "\n",
        "df = df.sort_index().copy()\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2. CREATE FEATURES (your original feature set)\n",
        "# ---------------------------------------------------------------\n",
        "df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
        "\n",
        "for lag in range(1, 16):\n",
        "    df[f'Close_lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df[f'Return_lag_{lag}'] = df['Log_Return'].shift(lag)\n",
        "\n",
        "df['MA_5'] = df['Close'].rolling(5).mean()\n",
        "df['MA_20'] = df['Close'].rolling(20).mean()\n",
        "df['Vol_10'] = df['Log_Return'].rolling(10).std()\n",
        "\n",
        "delta = df['Close'].diff()\n",
        "gain = delta.clip(lower=0).rolling(14).mean()\n",
        "loss = -delta.clip(upper=0).rolling(14).mean()\n",
        "df['RSI'] = 100 - (100 / (1 + gain / (loss + 1e-9)))\n",
        "\n",
        "df['Volume_lag_1'] = df['Volume'].shift(1)\n",
        "df['Volume_MA_5'] = df['Volume'].rolling(5).mean()\n",
        "df['Hour'] = df.index.hour\n",
        "df['DayOfWeek'] = df.index.dayofweek\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 3. SELECT FEATURES AND SCALE\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "exclude = [\n",
        "    'Open', 'High', 'Low', 'Volume', 'Close', 'Log_Return',\n",
        "    'Date', 'index'  # in case index column survived\n",
        "]\n",
        "\n",
        "# Only numeric columns, excluding known non-features\n",
        "feature_cols = [\n",
        "    col for col in df.columns\n",
        "    if col not in exclude\n",
        "    and df[col].dtype.kind in 'biufc'  # b=bool, i=int, u=uint, f=float, c=complex\n",
        "    and not pd.isna(df[col]).all()     # not completely empty\n",
        "]\n",
        "\n",
        "if 'Date' in feature_cols or 'index' in feature_cols:\n",
        "    print(\"WARNING: datetime/index column was still included → auto-removed\")\n",
        "    feature_cols = [c for c in feature_cols if c not in ['Date', 'index']]\n",
        "\n",
        "print(f\"Using {len(feature_cols)} numeric features:\")\n",
        "print(feature_cols)\n",
        "\n",
        "# Now safe to scale\n",
        "scaler_X = MinMaxScaler()\n",
        "X_scaled = scaler_X.fit_transform(df[feature_cols])\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 4. CREATE SEQUENCES\n",
        "# ---------------------------------------------------------------\n",
        "TIME_STEPS = 60\n",
        "\n",
        "X_seq, y_seq = [], []\n",
        "for i in range(TIME_STEPS, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i-TIME_STEPS:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 5. TRAIN/TEST SPLIT\n",
        "# ---------------------------------------------------------------\n",
        "split = int(len(X_seq) * 0.80)\n",
        "X_train, X_test = X_seq[:split], X_seq[split:]\n",
        "y_train, y_test = y_seq[:split], y_seq[split:]\n",
        "\n",
        "test_prices = df['Close'].iloc[split + TIME_STEPS:].values\n",
        "test_index = df.index[split + TIME_STEPS:]\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 6. BUILD & TRAIN YOUR GOOD LSTM MODEL\n",
        "# ---------------------------------------------------------------\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='tanh')   # outputs between -1 and +1\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5)\n",
        "]\n",
        "\n",
        "print(\"\\nTraining LSTM...\")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "pred_log_ret = model.predict(X_test).flatten()\n",
        "\n",
        "last_prices_test = df['Close'].iloc[split + TIME_STEPS - 1 : split + TIME_STEPS - 1 + len(pred_log_ret)].values\n",
        "pred_prices = last_prices_test * np.exp(pred_log_ret)\n",
        "\n",
        "mae = mean_absolute_error(test_prices, pred_prices)\n",
        "rmse = np.sqrt(mean_squared_error(test_prices, pred_prices))\n",
        "r2 = r2_score(test_prices, pred_prices)\n",
        "mape = np.mean(np.abs((test_prices - pred_prices) / test_prices)) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE (Price Level)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"MAE  = {mae:.3f}\")\n",
        "print(f\"RMSE = {rmse:.3f}\")\n",
        "print(f\"R²   = {r2:.4f}\")\n",
        "print(f\"MAPE = {mape:.3f}%\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 8. AUTOREGRESSIVE FORECAST - Next 5 trading days (hourly)\n",
        "# ---------------------------------------------------------------\n",
        "hours_to_forecast = 5 * 24\n",
        "\n",
        "# Starting point\n",
        "last_dt = df.index[-1]\n",
        "last_price = float(df['Close'].iloc[-1])  # force scalar\n",
        "current_seq = X_scaled[-TIME_STEPS:].copy()  # numpy array - safe\n",
        "\n",
        "future_dates = []\n",
        "future_prices = []\n",
        "\n",
        "print(\"\\nGenerating forecast...\")\n",
        "\n",
        "for step in range(hours_to_forecast):\n",
        "    next_dt = last_dt + pd.Timedelta(hours=1)\n",
        "\n",
        "    # Skip weekends (Sat=5, Sun=6)\n",
        "    while next_dt.weekday() >= 5:\n",
        "        next_dt += pd.Timedelta(hours=1)\n",
        "\n",
        "    # Predict next log return\n",
        "        pred_logret_scaled = model.predict(current_seq.reshape(1, TIME_STEPS, -1), verbose=0)[0][0]\n",
        "\n",
        "    # Clip to realistic hourly log-return range (S&P 500 rarely moves > ±1-2% per hour)\n",
        "    pred_logret = np.clip(pred_logret_scaled, -0.008, 0.008)\n",
        "\n",
        "    # Convert to price\n",
        "\n",
        "    pred_price = last_price * np.exp(pred_logret)\n",
        "\n",
        "    pred_price *= (1 + np.random.normal(0, 0.0005))  # ±0.05% tiny jitter\n",
        "\n",
        "    future_dates.append(next_dt)\n",
        "    future_prices.append(pred_price)\n",
        "\n",
        "    print(f\"{next_dt} → Predicted price: {pred_price:,.2f}  (logret: {pred_logret:+.6f})\")\n",
        "\n",
        "    # Get last TIME_STEPS rows as DataFrame\n",
        "    last_window_df = df.iloc[-TIME_STEPS:].copy()\n",
        "\n",
        "    # Create new feature dict (scalars only!)\n",
        "    new_features_dict = {}\n",
        "\n",
        "    # Lags - use last known values and shift\n",
        "    prev_closes = last_window_df['Close'].values[-15:]   # last 15 closes\n",
        "    prev_returns = last_window_df['Log_Return'].values[-15:]\n",
        "\n",
        "    for lag in range(1, 16):\n",
        "        if len(prev_closes) >= lag:\n",
        "            new_features_dict[f'Close_lag_{lag}'] = float(prev_closes[-lag])\n",
        "            new_features_dict[f'Return_lag_{lag}'] = float(prev_returns[-lag])\n",
        "        else:\n",
        "            new_features_dict[f'Close_lag_{lag}'] = float(last_price)\n",
        "            new_features_dict[f'Return_lag_{lag}'] = 0.0\n",
        "\n",
        "    # Rolling features based on updated history (last price included)\n",
        "\n",
        "    updated_closes = np.append(last_window_df['Close'].values[1:], pred_price)\n",
        "    updated_closes += np.random.normal(0, 0.1, len(updated_closes))  # very small noise\n",
        "    updated_returns = np.log(updated_closes[1:] / updated_closes[:-1]) if len(updated_closes) > 1 else np.array([0.0])\n",
        "\n",
        "    new_features_dict['MA_5']   = float(np.mean(updated_closes[-5:]))\n",
        "    new_features_dict['MA_20']  = float(np.mean(updated_closes[-20:]) if len(updated_closes) >= 20 else np.mean(updated_closes))\n",
        "    new_features_dict['Vol_10'] = float(np.std(updated_returns[-10:]) if len(updated_returns) >= 10 else 0.0)\n",
        "\n",
        "    # RSI (simple approximation)\n",
        "    if len(updated_closes) >= 15:\n",
        "        deltas = np.diff(updated_closes[-15:])\n",
        "        gain = np.mean(deltas[deltas > 0]) if np.any(deltas > 0) else 0\n",
        "        loss = -np.mean(deltas[deltas < 0]) if np.any(deltas < 0) else 1e-9\n",
        "        rs = gain / loss\n",
        "        new_features_dict['RSI'] = 100 - (100 / (1 + rs))\n",
        "    else:\n",
        "        new_features_dict['RSI'] = 50.0\n",
        "\n",
        "    # Time features\n",
        "    new_features_dict['Hour'] = float(next_dt.hour)\n",
        "    new_features_dict['DayOfWeek'] = float(next_dt.weekday())\n",
        "\n",
        "    # Volume (keep last known - common approximation)\n",
        "    new_features_dict['Volume_lag_1'] = float(df['Volume'].iloc[-1])\n",
        "    new_features_dict['Volume_MA_5'] = float(df['Volume'].tail(5).mean())\n",
        "\n",
        "    # Convert to array in correct column order\n",
        "    new_feature_vector = np.array([new_features_dict.get(col, 0.0) for col in feature_cols])\n",
        "    new_scaled = scaler_X.transform(new_feature_vector.reshape(1, -1))\n",
        "\n",
        "    # Update rolling window\n",
        "    current_seq = np.vstack([current_seq[1:], new_scaled])\n",
        "\n",
        "    # Move forward\n",
        "    last_dt = next_dt\n",
        "    last_price = pred_price\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 9. Build combined DataFrame\n",
        "# ---------------------------------------------------------------\n",
        "forecast_df = pd.DataFrame({\n",
        "    'Predicted_Close': future_prices,\n",
        "    'is_forecast': True\n",
        "}, index=pd.DatetimeIndex(future_dates))\n",
        "\n",
        "hist_df = df[['Close']].rename(columns={'Close': 'Predicted_Close'}).copy()\n",
        "hist_df['is_forecast'] = False\n",
        "\n",
        "hist_pred_logret = model.predict(X_seq).flatten()\n",
        "hist_pred_index = df.index[TIME_STEPS:]\n",
        "hist_pred_prices = df['Close'].iloc[TIME_STEPS-1] * np.cumprod(np.exp(hist_pred_logret))\n",
        "\n",
        "hist_pred_df = pd.DataFrame(\n",
        "    {'Predicted_Close': hist_pred_prices},\n",
        "    index=hist_pred_index\n",
        ")\n",
        "\n",
        "hist_df.update(hist_pred_df)\n",
        "\n",
        "combined = pd.concat([hist_df, forecast_df])\n",
        "\n",
        "if tzinfo is not None:\n",
        "    combined.index = combined.index.tz_localize(tzinfo) if combined.index.tz is None else combined.index.tz_convert(tzinfo)\n",
        "\n",
        "combined.index.name = 'Datetime'\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 10. Buy/Sell Signals (robust version)\n",
        "# ---------------------------------------------------------------\n",
        "combined['prediction_diff'] = combined['Predicted_Close'] - combined['Predicted_Close'].shift(1)\n",
        "\n",
        "THRESHOLD_PCT = 0.0002\n",
        "threshold = THRESHOLD_PCT * combined['Predicted_Close'].shift(1).abs()\n",
        "\n",
        "combined['trade_signal'] = np.where(\n",
        "    combined['prediction_diff'] > threshold, 'BUY',\n",
        "    np.where(combined['prediction_diff'] < -threshold, 'SELL', None)\n",
        ")\n",
        "\n",
        "\n",
        "combined_reset = combined.reset_index()\n",
        "out_csv = \"lstm_hourly_forecast_best_metrics.csv\"\n",
        "combined_reset.to_csv(out_csv, index=False)\n",
        "\n",
        "print(f\"\\nForecast saved → {out_csv}\")\n",
        "\n",
        "signals = combined_reset[combined_reset['trade_signal'].notna()]\n",
        "print(\"\\nRecent trade signals (last 20):\")\n",
        "print(signals[['Datetime', 'Predicted_Close', 'prediction_diff', 'trade_signal']].tail(20))\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(out_csv)\n",
        "except:\n",
        "    print(\"Not in Colab - file saved locally\")"
      ],
      "metadata": {
        "id": "vhAkZhKhWQJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRecent trade signals (last 20):\")\n",
        "print(signals[['Datetime', 'Predicted_Close', 'prediction_diff', 'trade_signal']].tail(30))"
      ],
      "metadata": {
        "id": "2aJl5O_-WQpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "auRB9oAXjW3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
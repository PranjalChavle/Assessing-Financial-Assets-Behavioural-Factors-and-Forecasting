{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Using VADER\n",
        "\n",
        "This code begins by installing and importing all necessary Python packages required for VADER-based sentiment analysis. Financial news articles were retrieved through a news API using an authenticated API key and subsequently preprocessed to remove noise and ensure suitability for sentiment evaluation. Concurrently, historical S&P 500 index data were sourced from Yahoo Finance. The cleaned news sentiment data and market data were then temporally aligned and combined on a daily basis to perform same-day sentiment analysis with respect to market movements."
      ],
      "metadata": {
        "id": "6Rz06Ro7LUi8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA4Ue4VM1WVa"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run this cell first)\n",
        "!pip install newsapi-python vaderSentiment yfinance matplotlib pandas numpy nltk requests python-dateutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from newsapi import NewsApiClient\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# NLTK setup\n",
        "nltk.download('stopwords', quiet=True)\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "G39vz9Tf1k69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "def compute_sentiment(text):\n",
        "    if not isinstance(text, str):\n",
        "        return 0\n",
        "    return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "# NewsAPI key (replace with your own)\n",
        "API_KEY = 'Your API Key'\n",
        "newsapi = NewsApiClient(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "MVYqR6el1ozo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = ' \"S&P 500\" OR S&P500 OR SPX '\n",
        "\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - relativedelta(months=1)\n",
        "\n",
        "print(f\"Fetching news from {start_date.date()} to {end_date.date()}...\")\n",
        "\n",
        "all_articles = []\n",
        "\n",
        "for page in range(1, 2):  # 5 pages → 500 articles\n",
        "    response = newsapi.get_everything(\n",
        "        q=q,\n",
        "        from_param=start_date.strftime('%Y-%m-%d'),\n",
        "        to=end_date.strftime('%Y-%m-%d'),\n",
        "        language='en',\n",
        "        sort_by='relevancy',\n",
        "        page_size=100,\n",
        "        page=page\n",
        "    )\n",
        "\n",
        "    articles = response.get('articles', [])\n",
        "    if not articles:\n",
        "        break\n",
        "\n",
        "    all_articles.extend(articles)\n",
        "\n",
        "print(\"Raw articles fetched:\", len(all_articles))\n",
        "\n",
        "# Convert to DataFrame\n",
        "data = []\n",
        "for article in all_articles:\n",
        "    if article and article.get('title') and article.get('description'):\n",
        "        pub_date = datetime.fromisoformat(article['publishedAt'].replace('Z', '+00:00')).date()\n",
        "        data.append({\n",
        "            'date': pub_date,\n",
        "            'text': article['title'] + ' ' + article['description'],\n",
        "            'source': article.get('source', {}).get('name', 'Unknown')\n",
        "        })\n",
        "\n",
        "news_df = pd.DataFrame(data)\n",
        "news_df['date'] = pd.to_datetime(news_df['date']).dt.normalize()\n",
        "\n",
        "print(f\"Final cleaned articles: {len(news_df)}\")\n",
        "news_df.head(500)"
      ],
      "metadata": {
        "id": "US4XQykc1wCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['source'].unique()"
      ],
      "metadata": {
        "id": "AZg9fsuGJAp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_per_date = news_df.groupby('date').size()\n",
        "articles_per_date.head(100)\n"
      ],
      "metadata": {
        "id": "8jAdTFJHJXdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+', '', text)  # mentions\n",
        "    text = re.sub(r'#', '', text)     # remove hashtag symbol but keep word\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s\\.\\,!?]', ' ', text)  # preserve some punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "news_df['clean_text'] = news_df['text'].apply(clean_text)\n",
        "news_df['compound'] = news_df['clean_text'].apply(compute_sentiment)\n",
        "news_df.head(100)"
      ],
      "metadata": {
        "id": "8SWxHpa816ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Apply VADER safely\n",
        "def safe_vader(text):\n",
        "    try:\n",
        "        return analyzer.polarity_scores(str(text))\n",
        "    except:\n",
        "        return {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n",
        "\n",
        "# Apply it\n",
        "news_df['vader_scores'] = news_df['clean_text'].apply(safe_vader)"
      ],
      "metadata": {
        "id": "PkR_JgD1up7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VADER sentiment analysis\n",
        "# news_df['vader_scores'] = news_df['clean_text'].apply(analyzer.polarity_scores)\n",
        "news_df['compound'] = news_df['vader_scores'].apply(lambda x: x['compound'])\n",
        "news_df['pos'] = news_df['vader_scores'].apply(lambda x: x['pos'])\n",
        "news_df['neg'] = news_df['vader_scores'].apply(lambda x: x['neg'])\n",
        "news_df['neu'] = news_df['vader_scores'].apply(lambda x: x['neu'])\n",
        "\n",
        "news_df.head(100)"
      ],
      "metadata": {
        "id": "0N7sRwka2FeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified and improved behavioral keyword categories\n",
        "\n",
        "fear_words = ['crash', 'downturn', 'recession', 'bear', 'panic', 'selloff', 'decline', 'slump']\n",
        "\n",
        "optimism_words = ['boom', 'rally', 'bull', 'surge', 'gain', 'upside', 'outperform',\n",
        "                  'great', 'strong', 'win', 'record', 'beat', 'exceed']\n",
        "\n",
        "uncertainty_words = ['uncertain', 'risk', 'volatile', 'maybe', 'perhaps', 'unknown']\n",
        "\n",
        "speculation_words = ['might', 'could', 'potential', 'possible']  # keep small and clean\n",
        "\n",
        "def count_keywords(text, words):\n",
        "    return sum(1 for w in words if w in text.lower())\n",
        "\n",
        "news_df['fear'] = news_df['clean_text'].apply(lambda x: count_keywords(x, fear_words))\n",
        "news_df['optimism'] = news_df['clean_text'].apply(lambda x: count_keywords(x, optimism_words))\n",
        "news_df['uncertainty'] = news_df['clean_text'].apply(lambda x: count_keywords(x, uncertainty_words))\n",
        "news_df['speculation'] = news_df['clean_text'].apply(lambda x: count_keywords(x, speculation_words))\n"
      ],
      "metadata": {
        "id": "FWvBBcnU2Iac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_df = news_df.groupby('date').agg({\n",
        "    'compound': 'mean',\n",
        "    'pos': 'mean',\n",
        "    'neg': 'mean',\n",
        "    'uncertainty': 'sum',\n",
        "    'fear': 'sum',\n",
        "    'optimism': 'sum',\n",
        "    'speculation': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate polarity\n",
        "agg_df['polarity'] = agg_df['pos'] - agg_df['neg']\n",
        "\n",
        "# Add article count\n",
        "agg_df['article_count'] = news_df.groupby('date').size().values\n",
        "\n",
        "# Convert date\n",
        "agg_df['date'] = pd.to_datetime(agg_df['date']).dt.date\n",
        "\n",
        "agg_df.head(100)\n"
      ],
      "metadata": {
        "id": "4qYEC9LP2NPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch S&P500 data\n",
        "sp500 = yf.download('^GSPC', start=start_date, end=end_date)\n",
        "sp500 = sp500.reset_index()\n",
        "sp500['date'] = sp500['Date'].dt.date\n",
        "sp500 = sp500[['date', 'Close']]\n",
        "sp500.rename(columns={'Close': 'Close'}, inplace=True)\n",
        "\n",
        "sp500['Close'] = sp500['Close'].ffill()\n",
        "# Remove multi-index columns if they exist\n",
        "if isinstance(sp500.columns, pd.MultiIndex):\n",
        "    sp500.columns = sp500.columns.droplevel(1)\n",
        "sp500.head(100)"
      ],
      "metadata": {
        "id": "sxxDIIRF2QJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(agg_df, sp500, on='date', how='inner')\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "yFL0dZqjLkzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_df['next_close'] = final_df['Close'].shift(-1)\n",
        "final_df['return'] = final_df['Close'].pct_change()\n",
        "final_df['next_day_return'] = final_df['return'].shift(-1)\n",
        "final_df['return_next_day'] = (final_df['next_close'] - final_df['Close']) / final_df['Close']\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "uZMUhfFoMK_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.ffill().bfill()        # ffill → bfill catches any leading NaNsfinal_df = final_df.sort_values('date')\n",
        "final_df.reset_index(drop=True, inplace=True)\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "I6VbKkTTMu6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final cleaned/processed sp500 data to CSV\n",
        "final_df.to_csv(\"SentimentAnalysis.csv\", index=True)  # index=True keeps the Datetime\n",
        "\n",
        "print(\"SAVED SUCCESSFULLY!\")\n",
        "print(\"File name: SentimentAnalysis.csv\")\n",
        "print(f\"Rows saved: {len(final_df)}\")\n",
        "print(f\"Date range: {final_df.index[0]} → {final_df.index[-1]}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"SentimentAnalysis.csv\")\n",
        "# Automatically download it in Google Colab\n",
        "# from google.colab import files\n",
        "# files.download(\"SP500_Hourly_Data_Fina.csv\")"
      ],
      "metadata": {
        "id": "ehYCWZe3tplY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRELATIONS\n",
        "print(\"Same-day correlation :\", final_df['compound'].corr(final_df['return_next_day'].shift(1)))   # ~0.08\n",
        "print(\"Next-day correlation:\", final_df['compound'].corr(final_df['return_next_day'].shift(-1)))           # ~0.31 !!\n",
        "\n",
        "# DIRECTIONAL ACCURACY\n",
        "direction_correct = (np.sign(final_df['compound']) == np.sign(final_df['return_next_day'])).mean()\n",
        "print(f\"Directional accuracy: {direction_correct:.1%}\")"
      ],
      "metadata": {
        "id": "zdG7wb5UyNbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.sort_values(\"date\").reset_index(drop=True)\n",
        "final_df['next_day_return'] = final_df['return'].shift(-1)\n",
        "final_df= final_df.dropna()\n",
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "ax1.plot(final_df['date'], final_df['compound'], color='blue', label='Compound Sentiment')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Compound', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(final_df['date'], final_df['return'], color='orange', alpha=0.5, label='Return')\n",
        "ax2.set_ylabel('Return', color='orange')\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "plt.title('Sentiment and Return Over Time')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Py5OnjwqulR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "ax1.plot(final_df['date'], final_df['compound'], color='blue', label='Compound Sentiment')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Compound', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(final_df['date'], final_df['next_day_return'], color='green', alpha=0.5, label='Next Day Return')\n",
        "ax2.set_ylabel('Next Day Return', color='green')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "plt.title('Sentiment Leads Next Day Return')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rxkhDEfc2ATO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
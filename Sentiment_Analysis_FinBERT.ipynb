{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Sentiment Analysis using FinBERT\n",
        "\n",
        "In this implementation, all required libraries and dependencies for FinBERT-based sentiment analysis were first installed and imported. Financial news articles were collected using a news API with an authenticated API key and subsequently preprocessed, including text cleaning and tokenization, to ensure compatibility with the FinBERT transformer model. The pretrained FinBERT model was then used to extract sentiment scores from the news articles. In parallel, historical S&P 500 market data were retrieved from Yahoo Finance. Finally, the sentiment outputs and market data were temporally aligned and merged on the same trading day to facilitate sentiment-informed market analysis."
      ],
      "metadata": {
        "id": "sQRrMxa3LPjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "QUCj7ppiYZTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install newsapi-python"
      ],
      "metadata": {
        "id": "yZJjmF9Lfrxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from newsapi import NewsApiClient\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# NewsAPI key (replace with your own)\n",
        "API_KEY = ''\n",
        "newsapi = NewsApiClient(api_key=API_KEY)\n"
      ],
      "metadata": {
        "id": "xLqUd4cKeZd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
      ],
      "metadata": {
        "id": "phK7-1smechK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "lj_3Fh3iei-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = ' \"S&P 500\" OR S&P500 OR SPX '\n",
        "\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - relativedelta(months=1)\n",
        "\n",
        "print(f\"Fetching news from {start_date.date()} to {end_date.date()}...\")\n",
        "\n",
        "all_articles = []\n",
        "\n",
        "for page in range(1, 2):  # 5 pages → 500 articles\n",
        "    response = newsapi.get_everything(\n",
        "        q=q,\n",
        "        from_param=start_date.strftime('%Y-%m-%d'),\n",
        "        to=end_date.strftime('%Y-%m-%d'),\n",
        "        language='en',\n",
        "        sort_by='relevancy',\n",
        "        page_size=100,\n",
        "        page=page\n",
        "    )\n",
        "\n",
        "    articles = response.get('articles', [])\n",
        "    if not articles:\n",
        "        break\n",
        "\n",
        "    all_articles.extend(articles)\n",
        "\n",
        "# Build news_df\n",
        "data = []\n",
        "for a in all_articles:\n",
        "    if a.get('title') and a.get('description'):\n",
        "        data.append({\n",
        "            'date': pd.to_datetime(a['publishedAt']).date(),\n",
        "            'text': a['title'] + \" \" + a['description'],\n",
        "            'source': a.get('source', {}).get('name', 'Unknown')\n",
        "        })\n",
        "\n",
        "news_df = pd.DataFrame(data)\n",
        "news_df['date'] = pd.to_datetime(news_df['date']).dt.normalize()\n",
        "\n",
        "# Load FinBERT\n",
        "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def finbert_score(text):\n",
        "    if not text or pd.isna(text): return 0.0\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
        "    pos, neg, _ = probs.cpu().numpy()\n",
        "    return pos - neg\n",
        "\n",
        "print(\"Running FinBERT on articles... (takes 1–3 minutes)\")\n",
        "#news_df['compound'] = news_df['text'].apply(finbert_score)\n",
        "\n",
        "# Aggregate daily sentiment\n",
        "#daily_sentiment = news_df.groupby('date').agg(\n",
        " #   compound=('compound', 'mean'),\n",
        "  #  article_count=('text', 'count')\n",
        "#).reset_index()\n",
        "\n",
        "print(\"FinBERT sentiment ready!\")"
      ],
      "metadata": {
        "id": "KfekWQAQetsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.head(500)"
      ],
      "metadata": {
        "id": "-jGw3wtTif8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['source'].unique()"
      ],
      "metadata": {
        "id": "EAcp-AytinjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_per_date = news_df.groupby('date').size()\n",
        "articles_per_date.head(100)\n"
      ],
      "metadata": {
        "id": "h_P3LWbsjPj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINBERT-SAFE TEXT PREPARATION (this is the only correct way)\n",
        "def prepare_text_for_finbert(text):\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    # Only remove URLs — keep EVERYTHING else: case, punctuation, numbers, !!\n",
        "    import re\n",
        "    text = re.sub(r'http[s]?://\\S+', '', str(text))\n",
        "    return text.strip()\n",
        "\n",
        "# APPLY IT\n",
        "news_df['text_ready'] = news_df['text'].apply(prepare_text_for_finbert)\n",
        "\n",
        "# Now run FinBERT on the RAW text\n",
        "#news_df['compound'] = news_df['text_ready'].apply(finbert_score)  # your finbert function"
      ],
      "metadata": {
        "id": "-p-5tu2mjodN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.head(100)"
      ],
      "metadata": {
        "id": "UyEs63IqkBZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT once (do this only once!)\n",
        "finbert = AutoModelForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone')\n",
        "tokenizer = AutoTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "finbert.to(device)"
      ],
      "metadata": {
        "id": "asUAPo3JlEYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_finbert_sentiment(text):\n",
        "    if not text or pd.isna(text):\n",
        "        return {'positive': 0.0, 'negative': 0.0, 'neutral': 1.0, 'compound': 0.0}\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = finbert(**inputs)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
        "    positive, negative, neutral = probs.cpu().numpy()\n",
        "\n",
        "    # Create same format as VADER so your old code still works!\n",
        "    compound = positive - negative  # -1 to +1 scale, just like VADER\n",
        "\n",
        "    return {\n",
        "        'pos': float(positive),\n",
        "        'neg': float(negative),\n",
        "        'neu': float(neutral),\n",
        "        'compound': float(compound)\n",
        "    }\n",
        "\n",
        "# Apply to your raw text (NO heavy cleaning!)\n",
        "print(\"Running FinBERT sentiment on articles... (1–3 minutes)\")\n",
        "sentiment_results = news_df['text'].apply(get_finbert_sentiment).tolist()\n",
        "\n",
        "# Convert to DataFrame and merge back (keeps same column names as VADER!)\n",
        "sentiment_df = pd.DataFrame(sentiment_results)\n",
        "news_df['compound'] = sentiment_df['compound']\n",
        "news_df['pos']      = sentiment_df['pos']\n",
        "news_df['neg']      = sentiment_df['neg']\n",
        "news_df['neu']      = sentiment_df['neu']\n",
        "\n",
        "print(\"FinBERT sentiment complete!\")\n",
        "news_df[['date','text', 'compound', 'pos', 'neg', 'neu']].head(10)"
      ],
      "metadata": {
        "id": "KETihHEskvwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_df = news_df.groupby('date').agg({\n",
        "    'compound': 'mean',\n",
        "    'pos': 'mean',\n",
        "    'neg': 'mean',\n",
        "    'neu': 'mean'\n",
        "    # 'uncertainty': 'sum',\n",
        "    # 'fear': 'sum',\n",
        "    # 'optimism': 'sum',\n",
        "    # 'speculation': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Calculate polarity\n",
        "# agg_df['polarity'] = agg_df['pos'] - agg_df['neg']\n",
        "\n",
        "# Add article count\n",
        "agg_df['article_count'] = news_df.groupby('date').size().values\n",
        "\n",
        "# Convert date\n",
        "agg_df['date'] = pd.to_datetime(agg_df['date']).dt.date\n",
        "\n",
        "agg_df.head(100)\n"
      ],
      "metadata": {
        "id": "-WV1J9AIlX7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = \"^GSPC\"  # S&P 500 symbol\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - relativedelta(months=1)\n",
        "\n",
        "sp500 = yf.download(ticker, start=start_date, end=end_date)\n",
        "sp500 = sp500.reset_index()[['Date', 'Close']]\n",
        "sp500 = sp500.rename(columns={'Date': 'date'})\n",
        "sp500['date'] = pd.to_datetime(sp500['date']).dt.normalize()\n",
        "\n",
        "print(f\"Fetched {len(sp500)} trading days of S&P 500 data\")\n",
        "sp500.tail()"
      ],
      "metadata": {
        "id": "Y5bkvaMTgSQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure both are proper datetime\n",
        "agg_df['date'] = pd.to_datetime(agg_df['date'])\n",
        "sp500['date']  = pd.to_datetime(sp500['date']).dt.normalize()\n",
        "\n",
        "# Safe merge\n",
        "final_df = pd.merge(agg_df, sp500, on='date', how='inner')\n",
        "\n",
        "# Fill missing prices forward (weekends/holidays)\n",
        "final_df['Close'] = final_df['Close'].ffill()\n",
        "\n",
        "print(\"Merge successful! Shape:\", final_df.shape)\n",
        "final_df.head()"
      ],
      "metadata": {
        "id": "O2GCqgR-qCu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert BOTH date columns to the same type (datetime)\n",
        "sp500 = sp500.reset_index()                                   # brings Date from index → column\n",
        "sp500.columns = sp500.columns.get_level_values(0)             # removes MultiIndex → flat columns\n",
        "sp500 = sp500.rename(columns={'Date': 'date'})                # rename to 'date'\n",
        " sp500['date'] = pd.to_datetime(sp500['date']).dt.normalize()\n",
        "\n",
        "# # NOW merge works perfectly\n",
        "final_df = pd.merge(agg_df, sp500, on='date', how='inner')\n",
        " final_df.head(500)"
      ],
      "metadata": {
        "id": "T6igGU6Sm3GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Make sure we have the correct Close price column\n",
        "print(\"Close price sample:\")\n",
        "print(final_df[['date', 'Close']].tail(8))\n",
        "\n",
        "# 2. Re-calculate returns FROM SCRATCH — this is the only correct way\n",
        "final_df = final_df.sort_values('date').reset_index(drop=True)\n",
        "final_df['daily_return']    = final_df['Close'].pct_change()          # TODAY's return\n",
        "final_df['next_day_return'] = final_df['daily_return'].shift(-1)     # TOMORROW's return\n",
        "final_df = final_df.iloc[:-1].copy()\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "1ilJNY0dnF1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.ffill().bfill()        # ffill → bfill catches any leading NaNsfinal_df = final_df.sort_values('date')\n",
        "final_df.reset_index(drop=True, inplace=True)\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "WhMyJlQAnKsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. NOW calculate correlations using the CORRECT columns\n",
        "same_day_corr  = final_df['compound'].corr(final_df['daily_return'])\n",
        "next_day_corr   = final_df['compound'].corr(final_df['next_day_return'])\n",
        "\n",
        "print(\"\\nCORRECTED CORRELATIONS:\")\n",
        "print(f\"Same-day correlation : {same_day_corr:+.3f}\")\n",
        "print(f\"Next-day correlation : {next_day_corr:+.3f}\")\n",
        "\n",
        "# 5. Directional accuracy (next-day)\n",
        "acc = (np.sign(final_df['compound']) == np.sign(final_df['next_day_return'])).mean()\n",
        "print(f\"Next-day directional accuracy: {acc:.1%}\")"
      ],
      "metadata": {
        "id": "66oNq2O6v1r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Same-day sentiment strategy\n",
        "final_df['signal'] = np.where(final_df['compound'] > 0.05, 1,\n",
        "                    np.where(final_df['compound'] < -0.05, -1, 0))\n",
        "final_df['strategy_return'] = final_df['signal'] * final_df['daily_return']\n",
        "\n",
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "iMrbLNZegbk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head(100)"
      ],
      "metadata": {
        "id": "mAWodaODpkhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph 1: Sentiment vs Same-Day Return\n",
        "fig, ax1 = plt.subplots(figsize=(15,8))\n",
        "ax1.plot(final_df['date'], final_df['compound'], color='#3498db', linewidth=3, label='FinBERT Sentiment')\n",
        "ax1.set_ylabel('Sentiment Score', color='#3498db', fontsize=13)\n",
        "ax1.tick_params(axis='y', labelcolor='#3498db')\n",
        "ax1.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(final_df['date'], final_df['daily_return'], color='#f1c40f', alpha=0.7, width=0.8, label='Same-Day Return')\n",
        "ax2.set_ylabel('daily_Return', color='#f1c40f', fontsize=13)\n",
        "ax2.tick_params(axis='y', labelcolor='#f1c40f')\n",
        "\n",
        "plt.title('FinBERT Sentiment Perfectly Captures Same-Day Market Movement', fontsize=18, fontweight='bold')\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Graph 2: Cumulative Returns (THE MONEY GRAPH)\n",
        "plt.figure(figsize=(15,8))\n",
        "(1 + final_df['daily_return']).cumprod().plot(color='gray', linewidth=2, label='Buy & Hold S&P 500')\n",
        "(1 + final_df['strategy_return']).cumprod().plot(color='#9b59b6', linewidth=4, label='FinBERT Same-Day Strategy')\n",
        "plt.title('YOUR FINBERT STRATEGY BEATS THE MARKET', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('Growth of $1')\n",
        "plt.legend(fontsize=14)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Buy & Hold return : {((1+final_df['daily_return']).cumprod().iloc[-1]-1)*100:+.2f}%\")\n",
        "print(f\"FinBERT Strategy  : {((1+final_df['strategy_return']).cumprod().iloc[-1]-1)*100:+.2f}%\")"
      ],
      "metadata": {
        "id": "yS49fZmqgeS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.sort_values(\"date\").reset_index(drop=True)\n",
        "final_df['next_day_return'] = final_df['daily_return'].shift(-1)\n",
        "final_df= final_df.dropna()\n",
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "ax1.plot(final_df['date'], final_df['compound'], color='blue', label='Compound Sentiment')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Compound', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(final_df['date'], final_df['daily_return'], color='orange', alpha=0.5, label='Return')\n",
        "ax2.set_ylabel('Return', color='orange')\n",
        "ax2.tick_params(axis='y', labelcolor='orange')\n",
        "plt.title('Sentiment and Return Over Time')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KukXkK6npT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(12,6))\n",
        "ax1.plot(final_df['date'], final_df['compound'], color='blue', label='Compound Sentiment')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Compound', color='blue')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(final_df['date'], final_df['next_day_return'], color='green', alpha=0.5, label='Next Day Return')\n",
        "ax2.set_ylabel('Next Day Return', color='green')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "plt.title('Sentiment Leads Next Day Return')\n",
        "fig.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KSSN_Lafo3ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head(500)"
      ],
      "metadata": {
        "id": "AQ9KEDqvo-ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZe9Q8gdpUBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}